{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics over twitter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify best and worst tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step one: from all the tweets, create the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, extract the data from a group into the DataFrame `tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pandas.options.display.max_colwidth = 500\n",
    "tweets = pandas.read_csv('twitter/g2.txt', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or else, get data from all the groups and create one DataFrame with them all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "import pandas\n",
    "\n",
    "pandas.options.display.max_colwidth = 500\n",
    "g1 = pandas.read_csv('twitter/g1.txt', sep='\\t', header=0)\n",
    "g2 = pandas.read_csv('twitter/g2.txt', sep='\\t', header=0)\n",
    "g3 = pandas.read_csv('twitter/g3.txt', sep='\\t', header=0)\n",
    "g4 = pandas.read_csv('twitter/g4.txt', sep='\\t', header=0)\n",
    "g5 = pandas.read_csv('twitter/g5.txt', sep='\\t', header=0)\n",
    "\n",
    "groups = [g1,g2,g3,g4,g5]\n",
    "\n",
    "tweets = pandas.concat(groups, ignore_index=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any content that is not in English (this could be done for any language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index = []\n",
    "for i,tweet in tweets.iterrows():\n",
    "    if('lang=\"en\"' not in tweet['FULL_TEXT_HTML']):\n",
    "        drop_index.append(i)\n",
    "        \n",
    "tweets = tweets.drop(drop_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the tweets according to likes and retweets, get the top 20% as `good_set`, and bottom as `bad_set`. <br>\n",
    "Finally, for each set add the tag `good` or `bad`, then join them and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "tweets = tweets.sort_values(by=['NLIKE','NRETWEET'], ascending=False)\n",
    "set_size = int(len(tweets)*0.2)\n",
    "\n",
    "good_set = tweets['FULL_TEXT'].head(set_size)\n",
    "bad_set = tweets['FULL_TEXT'].tail(set_size)\n",
    "\n",
    "good_set = [ (i, 'good') for i in good_set]\n",
    "bad_set = [ (i, 'bad') for i in bad_set]\n",
    "\n",
    "training_data = good_set + bad_set\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step two: clean the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the tokens along with their respective tweet's tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens = [(nltk.word_tokenize(tweet),tag) for tweet,tag in training_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stemming and lemmatization to the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stemmed = [porter_stemmer.stem(word) for t in tokens for word in t[0]]\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in stemmed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords, digits (not numbers) and punctuation, then create a `vocabulary` of the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords = set(stopwords.words('english') \n",
    "                + list(string.punctuation) \n",
    "                + list(string.digits) \n",
    "                + list(['“', '”', '’', '‘', '–', '…']))\n",
    "\n",
    "words = [word.lower() for word in lemmatized if word not in stopwords]\n",
    "\n",
    "vocabulary = [w[0] for w in nltk.FreqDist(words).most_common(3000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step three: generate sets for training and testing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Feature Set from `tokens` and `vocabulary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docFeatures(document):\n",
    "    doc_words = set(document)\n",
    "    features = {}\n",
    "    for word in vocabulary:\n",
    "        features['contains({})'.format(word)] = (word in doc_words)\n",
    "    return features\n",
    "\n",
    "feature_set = [(docFeatures(tweet), tag) for tweet,tag in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally create the Naive Bayes classifier, and test it with our Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(len(feature_set)*0.2)\n",
    "\n",
    "train_set, test_set = feature_set[x:], feature_set[:x]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# <br><br>Sentiment Analysis classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use nltk twitter samples to train the classifier, then we will try to classify our tweets from the museums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already done:\n",
    "\n",
    "```Python\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: get the tweets and define tokens and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the tweets and tokens from both the positive and negative set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "pos_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add for each tweet a `pos` or `neg` tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_tweets = [ (i, 'pos') for i in positive_tweets]\n",
    "negative_tweets = [ (i, 'neg') for i in negative_tweets]\n",
    "\n",
    "training_data = positive_tweets + negative_tweets\n",
    "\n",
    "pos_tokens = [ (i, 'pos') for i in pos_tokens]\n",
    "neg_tokens = [ (i, 'neg') for i in neg_tokens]\n",
    "\n",
    "tokens = pos_tokens + neg_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Stemming on each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End the cleaning process by removing stopwords, punctuation, digits, links and citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "pos_stemmed = [porter_stemmer.stem(word) \n",
    "               for t in pos_tokens for word in t[0]]\n",
    "neg_stemmed = [porter_stemmer.stem(word) \n",
    "               for t in neg_tokens for word in t[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pos_lemmatized = [wordnet_lemmatizer.lemmatize(word)\n",
    "                  for word in pos_stemmed]\n",
    "neg_lemmatized = [wordnet_lemmatizer.lemmatize(word) \n",
    "                  for word in neg_stemmed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End the cleaning process by removing stopwords, punctuation, digits, links and citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string, re\n",
    "\n",
    "lemmatized = pos_lemmatized + neg_lemmatized\n",
    "stopwords = set(stopwords.words('english') \n",
    "                + list(string.punctuation) \n",
    "                + list(string.digits))\n",
    "\n",
    "words = [word.lower() for word in lemmatized if word not in stopwords]\n",
    "\n",
    "for word in words:\n",
    "    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n",
    "    word = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", word)\n",
    "    \n",
    "vocabulary = [w[0] for w in nltk.FreqDist(words).most_common(5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: define the feature set from our Twitter Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the tweets, tokenize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "pandas.options.display.max_colwidth = 500\n",
    "g1 = pandas.read_csv('twitter/g1.txt', sep='\\t', header=0)\n",
    "g2 = pandas.read_csv('twitter/g2.txt', sep='\\t', header=0)\n",
    "g3 = pandas.read_csv('twitter/g3.txt', sep='\\t', header=0)\n",
    "g4 = pandas.read_csv('twitter/g4.txt', sep='\\t', header=0)\n",
    "g5 = pandas.read_csv('twitter/g5.txt', sep='\\t', header=0)\n",
    "\n",
    "groups = [g1]\n",
    "#groups = [g1,g2,g3,g4,g5]\n",
    "\n",
    "tweets = pandas.concat(groups, ignore_index=True)\n",
    "\n",
    "# Keep English tweets\n",
    "drop_index = []\n",
    "for i,tweet in tweets.iterrows():\n",
    "    if('lang=\"en\"' not in tweet['FULL_TEXT_HTML']):\n",
    "        drop_index.append(i)\n",
    "        \n",
    "tweets = tweets.drop(drop_index)\n",
    "\n",
    "testing_data = tweets['FULL_TEXT']\n",
    "\n",
    "testing_tokens = [(nltk.word_tokenize(tweet)) for tweet in testing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And create a feature set using the vocabulary defined in Step One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docFeatures(document):\n",
    "    doc_words = set(document)\n",
    "    features = {}\n",
    "    for word in vocabulary:\n",
    "        features['contains({})'.format(word)] = (word in doc_words)\n",
    "    return features\n",
    "\n",
    "testing_feature_set = [(docFeatures(tweet)) for tweet in testing_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Three: train and test the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle negation and create unigram features (applying a minimum frequency of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import *\n",
    "\n",
    "sent_analyzer = nltk.sentiment.SentimentAnalyzer()\n",
    "\n",
    "negation_words = sent_analyzer.all_words([mark_negation(doc) for doc in tokens])\n",
    "unigram_features = sent_analyzer.unigram_word_feats(negation_words, min_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Feature Set and split it for training and testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(len(training_data)*0.2)\n",
    "\n",
    "sent_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_features)\n",
    "\n",
    "feature_sets = sent_analyzer.apply_features(tokens)\n",
    "train_set, test_set = feature_sets[x:], feature_sets[:x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the Naive Bayes classifier and print all the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sent_analyzer.train(nltk.classify.NaiveBayesClassifier.train, train_set)\n",
    "for key,value in sorted(sent_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))\n",
    "    \n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Four: verify the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the classifier on top of our `testing_feature_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = [classifier.prob_classify(f) for f in testing_feature_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(confidence[i].max(),\n",
    "          confidence[i].prob(confidence[i].max()),\n",
    "          '\\n' + testing_data[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Note\n",
    "In the first part of this notebook we saw how to create a classifier, how to normalize our data, how to extract the features and divide the feature set for training and testing.\n",
    "<br>\n",
    "The second part focuses more on sentiment analysis, trying to classify the attitude (Positive or Negative) of the tweets we had. In order to do so, it was mandatory to train the model using structured data. Luckily nltk offered that...\n",
    "After training the classifier, and extracting our `testing_feature_set`, we could finally run the classification. Although those tweets weren't specifically for sentiment analysis, it was interesting to work on this second task, showing the power of these features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
